--------- beginning of main
01-26 11:01:51.105 D/LLAMA_CPP( 8726): ggml_vulkan: Found 1 Vulkan devices:
01-26 11:01:51.106 D/LLAMA_CPP( 8726): ggml_vulkan: 0 = Adreno (TM) 730 (Qualcomm Technologies Inc. Adreno Vulkan Driver) | uma: 1 | fp16: 1 | bf16: 0 | warp size: 64 | shared memory: 32768 | int dot: 0 | matrix cores: none
01-26 11:01:51.115 I/LLAMA_CPP( 8726): llama_model_load_from_file_impl: using device Vulkan0 (Adreno (TM) 730) (unknown id) - 7476 MiB free
01-26 11:01:51.169 E/LLAMA_CPP( 8726): llama_model_load: error loading model: tensor 'output.weight' data is not within the file bounds, model is corrupted or incomplete
01-26 11:01:51.169 E/LLAMA_CPP( 8726): llama_model_load_from_file_impl: failed to load model
01-26 11:01:51.169 I/LLAMA_CPP( 8726): llama_model_load_from_file_impl: using device Vulkan0 (Adreno (TM) 730) (unknown id) - 7476 MiB free
01-26 11:01:51.207 E/LLAMA_CPP( 8726): llama_model_load: error loading model: tensor 'output.weight' data is not within the file bounds, model is corrupted or incomplete
01-26 11:01:51.207 E/LLAMA_CPP( 8726): llama_model_load_from_file_impl: failed to load model
01-26 11:01:51.815 I/LLAMA_CPP( 8726): llama_model_load_from_file_impl: using device Vulkan0 (Adreno (TM) 730) (unknown id) - 7476 MiB free
01-26 11:01:51.853 E/LLAMA_CPP( 8726): llama_model_load: error loading model: tensor 'output.weight' data is not within the file bounds, model is corrupted or incomplete
01-26 11:01:51.853 E/LLAMA_CPP( 8726): llama_model_load_from_file_impl: failed to load model
01-26 11:01:51.853 I/LLAMA_CPP( 8726): llama_model_load_from_file_impl: using device Vulkan0 (Adreno (TM) 730) (unknown id) - 7476 MiB free
01-26 11:01:51.885 E/LLAMA_CPP( 8726): llama_model_load: error loading model: tensor 'output.weight' data is not within the file bounds, model is corrupted or incomplete
01-26 11:01:51.885 E/LLAMA_CPP( 8726): llama_model_load_from_file_impl: failed to load model
01-26 11:01:52.018 I/LLAMA_CPP( 8726): llama_model_load_from_file_impl: using device Vulkan0 (Adreno (TM) 730) (unknown id) - 7476 MiB free
01-26 11:01:52.071 E/LLAMA_CPP( 8726): llama_model_load: error loading model: tensor 'output.weight' data is not within the file bounds, model is corrupted or incomplete
01-26 11:01:52.071 E/LLAMA_CPP( 8726): llama_model_load_from_file_impl: failed to load model
01-26 11:01:52.071 I/LLAMA_CPP( 8726): llama_model_load_from_file_impl: using device Vulkan0 (Adreno (TM) 730) (unknown id) - 7476 MiB free
01-26 11:01:52.111 E/LLAMA_CPP( 8726): llama_model_load: error loading model: tensor 'output.weight' data is not within the file bounds, model is corrupted or incomplete
01-26 11:01:52.111 E/LLAMA_CPP( 8726): llama_model_load_from_file_impl: failed to load model
01-26 11:01:52.828 I/LLAMA_CPP( 8726): llama_model_load_from_file_impl: using device Vulkan0 (Adreno (TM) 730) (unknown id) - 7476 MiB free
01-26 11:01:52.837 I/LLAMA_CPP( 8726): llama_model_loader: loaded meta data with 23 key-value pairs and 112 tensors from /data/user/0/com.synapsenotes.ai/files/models/nomic-embed-text-v1.5.Q8_0.gguf (version GGUF V3 (latest))
01-26 11:01:52.837 I/LLAMA_CPP( 8726): llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
01-26 11:01:52.837 I/LLAMA_CPP( 8726): llama_model_loader: - kv   0:                       general.architecture str              = nomic-bert
01-26 11:01:52.837 I/LLAMA_CPP( 8726): llama_model_loader: - kv   1:                               general.name str              = nomic-embed-text-v1.5
01-26 11:01:52.837 I/LLAMA_CPP( 8726): llama_model_loader: - kv   2:                     nomic-bert.block_count u32              = 12
01-26 11:01:52.837 I/LLAMA_CPP( 8726): llama_model_loader: - kv   3:                  nomic-bert.context_length u32              = 2048
01-26 11:01:52.837 I/LLAMA_CPP( 8726): llama_model_loader: - kv   4:                nomic-bert.embedding_length u32              = 768
01-26 11:01:52.837 I/LLAMA_CPP( 8726): llama_model_loader: - kv   5:             nomic-bert.feed_forward_length u32              = 3072
01-26 11:01:52.837 I/LLAMA_CPP( 8726): llama_model_loader: - kv   6:            nomic-bert.attention.head_count u32              = 12
01-26 11:01:52.837 I/LLAMA_CPP( 8726): llama_model_loader: - kv   7:    nomic-bert.attention.layer_norm_epsilon f32              = 0.000000
01-26 11:01:52.837 I/LLAMA_CPP( 8726): llama_model_loader: - kv   8:                          general.file_type u32              = 7
01-26 11:01:52.837 I/LLAMA_CPP( 8726): llama_model_loader: - kv   9:                nomic-bert.attention.causal bool             = false
01-26 11:01:52.837 I/LLAMA_CPP( 8726): llama_model_loader: - kv  10:                    nomic-bert.pooling_type u32              = 1
01-26 11:01:52.837 I/LLAMA_CPP( 8726): llama_model_loader: - kv  11:                  nomic-bert.rope.freq_base f32              = 1000.000000
01-26 11:01:52.837 I/LLAMA_CPP( 8726): llama_model_loader: - kv  12:            tokenizer.ggml.token_type_count u32              = 2
01-26 11:01:52.837 I/LLAMA_CPP( 8726): llama_model_loader: - kv  13:                tokenizer.ggml.bos_token_id u32              = 101
01-26 11:01:52.837 I/LLAMA_CPP( 8726): llama_model_loader: - kv  14:                tokenizer.ggml.eos_token_id u32              = 102
01-26 11:01:52.837 I/LLAMA_CPP( 8726): llama_model_loader: - kv  15:                       tokenizer.ggml.model str              = bert
01-26 11:01:52.843 I/LLAMA_CPP( 8726): llama_model_loader: - kv  16:                      tokenizer.ggml.tokens arr[str,30522]   = ["[PAD]", "[unused0]", "[unused1]", "...
01-26 11:01:52.852 I/LLAMA_CPP( 8726): llama_model_loader: - kv  17:                      tokenizer.ggml.scores arr[f32,30522]   = [-1000.000000, -1000.000000, -1000.00...
01-26 11:01:52.853 I/LLAMA_CPP( 8726): llama_model_loader: - kv  18:                  tokenizer.ggml.token_type arr[i32,30522]   = [3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
01-26 11:01:52.853 I/LLAMA_CPP( 8726): llama_model_loader: - kv  19:            tokenizer.ggml.unknown_token_id u32              = 100
01-26 11:01:52.853 I/LLAMA_CPP( 8726): llama_model_loader: - kv  20:          tokenizer.ggml.seperator_token_id u32              = 102
01-26 11:01:52.853 I/LLAMA_CPP( 8726): llama_model_loader: - kv  21:            tokenizer.ggml.padding_token_id u32              = 0
01-26 11:01:52.853 I/LLAMA_CPP( 8726): llama_model_loader: - kv  22:               general.quantization_version u32              = 2
01-26 11:01:52.853 I/LLAMA_CPP( 8726): llama_model_loader: - type  f32:   51 tensors
01-26 11:01:52.853 I/LLAMA_CPP( 8726): llama_model_loader: - type q8_0:   61 tensors
01-26 11:01:52.853 I/LLAMA_CPP( 8726): print_info: file format = GGUF V3 (latest)
01-26 11:01:52.853 I/LLAMA_CPP( 8726): print_info: file type   = Q8_0
01-26 11:01:52.853 I/LLAMA_CPP( 8726): print_info: file size   = 138.65 MiB (8.51 BPW) 
01-26 11:01:52.859 D/LLAMA_CPP( 8726): init_tokenizer: initializing tokenizer for type 3
01-26 11:01:52.860 I/LLAMA_CPP( 8726): load: 0 unused tokens
01-26 11:01:52.861 D/LLAMA_CPP( 8726): load: control token:    102 '[SEP]' is not marked as EOG
01-26 11:01:52.861 D/LLAMA_CPP( 8726): load: control token:    101 '[CLS]' is not marked as EOG
01-26 11:01:52.861 D/LLAMA_CPP( 8726): load: control token:    100 '[UNK]' is not marked as EOG
01-26 11:01:52.861 D/LLAMA_CPP( 8726): load: control token:    103 '[MASK]' is not marked as EOG
01-26 11:01:52.861 D/LLAMA_CPP( 8726): load: control token:      0 '[PAD]' is not marked as EOG
01-26 11:01:52.862 W/LLAMA_CPP( 8726): load: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect
01-26 11:01:52.862 I/LLAMA_CPP( 8726): load: printing all EOG tokens:
01-26 11:01:52.862 I/LLAMA_CPP( 8726): load:   - 102 ('[SEP]')
01-26 11:01:52.862 I/LLAMA_CPP( 8726): load: special tokens cache size = 5
01-26 11:01:52.864 I/LLAMA_CPP( 8726): load: token to piece cache size = 0.2032 MB
01-26 11:01:52.864 I/LLAMA_CPP( 8726): print_info: arch                  = nomic-bert
01-26 11:01:52.864 I/LLAMA_CPP( 8726): print_info: vocab_only            = 0
01-26 11:01:52.864 I/LLAMA_CPP( 8726): print_info: no_alloc              = 0
01-26 11:01:52.864 I/LLAMA_CPP( 8726): print_info: n_ctx_train           = 2048
01-26 11:01:52.864 I/LLAMA_CPP( 8726): print_info: n_embd                = 768
01-26 11:01:52.864 I/LLAMA_CPP( 8726): print_info: n_embd_inp            = 768
01-26 11:01:52.864 I/LLAMA_CPP( 8726): print_info: n_layer               = 12
01-26 11:01:52.864 I/LLAMA_CPP( 8726): print_info: n_head                = 12
01-26 11:01:52.864 I/LLAMA_CPP( 8726): print_info: n_head_kv             = 12
01-26 11:01:52.864 I/LLAMA_CPP( 8726): print_info: n_rot                 = 64
01-26 11:01:52.864 I/LLAMA_CPP( 8726): print_info: n_swa                 = 0
01-26 11:01:52.864 I/LLAMA_CPP( 8726): print_info: is_swa_any            = 0
01-26 11:01:52.864 I/LLAMA_CPP( 8726): print_info: n_embd_head_k         = 64
01-26 11:01:52.864 I/LLAMA_CPP( 8726): print_info: n_embd_head_v         = 64
01-26 11:01:52.864 I/LLAMA_CPP( 8726): print_info: n_gqa                 = 1
01-26 11:01:52.864 I/LLAMA_CPP( 8726): print_info: n_embd_k_gqa          = 768
01-26 11:01:52.864 I/LLAMA_CPP( 8726): print_info: n_embd_v_gqa          = 768
01-26 11:01:52.864 I/LLAMA_CPP( 8726): print_info: f_norm_eps            = 1.0e-12
01-26 11:01:52.864 I/LLAMA_CPP( 8726): print_info: f_norm_rms_eps        = 0.0e+00
01-26 11:01:52.864 I/LLAMA_CPP( 8726): print_info: f_clamp_kqv           = 0.0e+00
01-26 11:01:52.864 I/LLAMA_CPP( 8726): print_info: f_max_alibi_bias      = 0.0e+00
01-26 11:01:52.864 I/LLAMA_CPP( 8726): print_info: f_logit_scale         = 0.0e+00
01-26 11:01:52.864 I/LLAMA_CPP( 8726): print_info: f_attn_scale          = 0.0e+00
01-26 11:01:52.864 I/LLAMA_CPP( 8726): print_info: n_ff                  = 3072
01-26 11:01:52.864 I/LLAMA_CPP( 8726): print_info: n_expert              = 0
01-26 11:01:52.864 I/LLAMA_CPP( 8726): print_info: n_expert_used         = 0
01-26 11:01:52.864 I/LLAMA_CPP( 8726): print_info: n_expert_groups       = 0
01-26 11:01:52.864 I/LLAMA_CPP( 8726): print_info: n_group_used          = 0
01-26 11:01:52.864 I/LLAMA_CPP( 8726): print_info: causal attn           = 0
01-26 11:01:52.864 I/LLAMA_CPP( 8726): print_info: pooling type          = 1
01-26 11:01:52.864 I/LLAMA_CPP( 8726): print_info: rope type             = 2
01-26 11:01:52.864 I/LLAMA_CPP( 8726): print_info: rope scaling          = linear
01-26 11:01:52.864 I/LLAMA_CPP( 8726): print_info: freq_base_train       = 1000.0
01-26 11:01:52.864 I/LLAMA_CPP( 8726): print_info: freq_scale_train      = 1
01-26 11:01:52.864 I/LLAMA_CPP( 8726): print_info: n_ctx_orig_yarn       = 2048
01-26 11:01:52.864 I/LLAMA_CPP( 8726): print_info: rope_yarn_log_mul     = 0.0000
01-26 11:01:52.864 I/LLAMA_CPP( 8726): print_info: rope_finetuned        = unknown
01-26 11:01:52.864 I/LLAMA_CPP( 8726): print_info: model type            = 137M
01-26 11:01:52.864 I/LLAMA_CPP( 8726): print_info: model params          = 136.73 M
01-26 11:01:52.864 I/LLAMA_CPP( 8726): print_info: general.name          = nomic-embed-text-v1.5
01-26 11:01:52.864 I/LLAMA_CPP( 8726): print_info: vocab type            = WPM
01-26 11:01:52.864 I/LLAMA_CPP( 8726): print_info: n_vocab               = 30522
01-26 11:01:52.864 I/LLAMA_CPP( 8726): print_info: n_merges              = 0
01-26 11:01:52.864 I/LLAMA_CPP( 8726): print_info: BOS token             = 101 '[CLS]'
01-26 11:01:52.864 I/LLAMA_CPP( 8726): print_info: EOS token             = 102 '[SEP]'
01-26 11:01:52.864 I/LLAMA_CPP( 8726): print_info: UNK token             = 100 '[UNK]'
01-26 11:01:52.864 I/LLAMA_CPP( 8726): print_info: SEP token             = 102 '[SEP]'
01-26 11:01:52.864 I/LLAMA_CPP( 8726): print_info: PAD token             = 0 '[PAD]'
01-26 11:01:52.864 I/LLAMA_CPP( 8726): print_info: MASK token            = 103 '[MASK]'
01-26 11:01:52.864 I/LLAMA_CPP( 8726): print_info: LF token              = 0 '[PAD]'
01-26 11:01:52.864 I/LLAMA_CPP( 8726): print_info: EOG token             = 102 '[SEP]'
01-26 11:01:52.864 I/LLAMA_CPP( 8726): print_info: max token length      = 21
01-26 11:01:52.864 I/LLAMA_CPP( 8726): load_tensors: loading model tensors, this can take a while... (mmap = false, direct_io = true)
01-26 11:01:52.928 D/LLAMA_CPP( 8726): load_tensors: layer   0 assigned to device CPU, is_swa = 0
01-26 11:01:52.928 D/LLAMA_CPP( 8726): load_tensors: layer   1 assigned to device CPU, is_swa = 0
01-26 11:01:52.928 D/LLAMA_CPP( 8726): load_tensors: layer   2 assigned to device CPU, is_swa = 0
01-26 11:01:52.928 D/LLAMA_CPP( 8726): load_tensors: layer   3 assigned to device CPU, is_swa = 0
01-26 11:01:52.928 D/LLAMA_CPP( 8726): load_tensors: layer   4 assigned to device CPU, is_swa = 0
01-26 11:01:52.928 D/LLAMA_CPP( 8726): load_tensors: layer   5 assigned to device CPU, is_swa = 0
01-26 11:01:52.928 D/LLAMA_CPP( 8726): load_tensors: layer   6 assigned to device CPU, is_swa = 0
01-26 11:01:52.928 D/LLAMA_CPP( 8726): load_tensors: layer   7 assigned to device CPU, is_swa = 0
01-26 11:01:52.928 D/LLAMA_CPP( 8726): load_tensors: layer   8 assigned to device CPU, is_swa = 0
01-26 11:01:52.928 D/LLAMA_CPP( 8726): load_tensors: layer   9 assigned to device CPU, is_swa = 0
01-26 11:01:52.928 D/LLAMA_CPP( 8726): load_tensors: layer  10 assigned to device CPU, is_swa = 0
01-26 11:01:52.928 D/LLAMA_CPP( 8726): load_tensors: layer  11 assigned to device CPU, is_swa = 0
01-26 11:01:52.928 D/LLAMA_CPP( 8726): load_tensors: layer  12 assigned to device CPU, is_swa = 0
01-26 11:01:52.928 D/LLAMA_CPP( 8726): create_tensor: loading tensor token_embd.weight
01-26 11:01:52.928 D/LLAMA_CPP( 8726): create_tensor: loading tensor token_types.weight
01-26 11:01:52.928 D/LLAMA_CPP( 8726): create_tensor: loading tensor token_embd_norm.weight
01-26 11:01:52.928 D/LLAMA_CPP( 8726): create_tensor: loading tensor token_embd_norm.bias
01-26 11:01:52.928 D/LLAMA_CPP( 8726): create_tensor: loading tensor blk.0.attn_qkv.weight
01-26 11:01:52.928 D/LLAMA_CPP( 8726): create_tensor: loading tensor blk.0.attn_output.weight
01-26 11:01:52.928 D/LLAMA_CPP( 8726): create_tensor: loading tensor blk.0.attn_output_norm.weight
01-26 11:01:52.928 D/LLAMA_CPP( 8726): create_tensor: loading tensor blk.0.attn_output_norm.bias
01-26 11:01:52.928 D/LLAMA_CPP( 8726): create_tensor: loading tensor blk.0.ffn_up.weight
01-26 11:01:52.928 D/LLAMA_CPP( 8726): create_tensor: loading tensor blk.0.ffn_down.weight
01-26 11:01:52.928 D/LLAMA_CPP( 8726): create_tensor: loading tensor blk.0.ffn_gate.weight
01-26 11:01:52.928 D/LLAMA_CPP( 8726): create_tensor: loading tensor blk.0.layer_output_norm.weight
01-26 11:01:52.928 D/LLAMA_CPP( 8726): create_tensor: loading tensor blk.0.layer_output_norm.bias
01-26 11:01:52.928 D/LLAMA_CPP( 8726): create_tensor: loading tensor blk.1.attn_qkv.weight
01-26 11:01:52.928 D/LLAMA_CPP( 8726): create_tensor: loading tensor blk.1.attn_output.weight
01-26 11:01:52.928 D/LLAMA_CPP( 8726): create_tensor: loading tensor blk.1.attn_output_norm.weight
01-26 11:01:52.928 D/LLAMA_CPP( 8726): create_tensor: loading tensor blk.1.attn_output_norm.bias
01-26 11:01:52.928 D/LLAMA_CPP( 8726): create_tensor: loading tensor blk.1.ffn_up.weight
01-26 11:01:52.928 D/LLAMA_CPP( 8726): create_tensor: loading tensor blk.1.ffn_down.weight
01-26 11:01:52.928 D/LLAMA_CPP( 8726): create_tensor: loading tensor blk.1.ffn_gate.weight
01-26 11:01:52.928 D/LLAMA_CPP( 8726): create_tensor: loading tensor blk.1.layer_output_norm.weight
01-26 11:01:52.928 D/LLAMA_CPP( 8726): create_tensor: loading tensor blk.1.layer_output_norm.bias
01-26 11:01:52.928 D/LLAMA_CPP( 8726): create_tensor: loading tensor blk.2.attn_qkv.weight
01-26 11:01:52.928 D/LLAMA_CPP( 8726): create_tensor: loading tensor blk.2.attn_output.weight
01-26 11:01:52.928 D/LLAMA_CPP( 8726): create_tensor: loading tensor blk.2.attn_output_norm.weight
01-26 11:01:52.928 D/LLAMA_CPP( 8726): create_tensor: loading tensor blk.2.attn_output_norm.bias
01-26 11:01:52.928 D/LLAMA_CPP( 8726): create_tensor: loading tensor blk.2.ffn_up.weight
01-26 11:01:52.928 D/LLAMA_CPP( 8726): create_tensor: loading tensor blk.2.ffn_down.weight
01-26 11:01:52.928 D/LLAMA_CPP( 8726): create_tensor: loading tensor blk.2.ffn_gate.weight
01-26 11:01:52.928 D/LLAMA_CPP( 8726): create_tensor: loading tensor blk.2.layer_output_norm.weight
01-26 11:01:52.928 D/LLAMA_CPP( 8726): create_tensor: loading tensor blk.2.layer_output_norm.bias
01-26 11:01:52.928 D/LLAMA_CPP( 8726): create_tensor: loading tensor blk.3.attn_qkv.weight
01-26 11:01:52.928 D/LLAMA_CPP( 8726): create_tensor: loading tensor blk.3.attn_output.weight
01-26 11:01:52.928 D/LLAMA_CPP( 8726): create_tensor: loading tensor blk.3.attn_output_norm.weight
01-26 11:01:52.928 D/LLAMA_CPP( 8726): create_tensor: loading tensor blk.3.attn_output_norm.bias
01-26 11:01:52.928 D/LLAMA_CPP( 8726): create_tensor: loading tensor blk.3.ffn_up.weight
01-26 11:01:52.928 D/LLAMA_CPP( 8726): create_tensor: loading tensor blk.3.ffn_down.weight
01-26 11:01:52.928 D/LLAMA_CPP( 8726): create_tensor: loading tensor blk.3.ffn_gate.weight
01-26 11:01:52.928 D/LLAMA_CPP( 8726): create_tensor: loading tensor blk.3.layer_output_norm.weight
01-26 11:01:52.929 D/LLAMA_CPP( 8726): create_tensor: loading tensor blk.3.layer_output_norm.bias
01-26 11:01:52.929 D/LLAMA_CPP( 8726): create_tensor: loading tensor blk.4.attn_qkv.weight
01-26 11:01:52.929 D/LLAMA_CPP( 8726): create_tensor: loading tensor blk.4.attn_output.weight
01-26 11:01:52.929 D/LLAMA_CPP( 8726): create_tensor: loading tensor blk.4.attn_output_norm.weight
01-26 11:01:52.929 D/LLAMA_CPP( 8726): create_tensor: loading tensor blk.4.attn_output_norm.bias
01-26 11:01:52.929 D/LLAMA_CPP( 8726): create_tensor: loading tensor blk.4.ffn_up.weight
01-26 11:01:52.929 D/LLAMA_CPP( 8726): create_tensor: loading tensor blk.4.ffn_down.weight
01-26 11:01:52.929 D/LLAMA_CPP( 8726): create_tensor: loading tensor blk.4.ffn_gate.weight
01-26 11:01:52.929 D/LLAMA_CPP( 8726): create_tensor: loading tensor blk.4.layer_output_norm.weight
01-26 11:01:52.929 D/LLAMA_CPP( 8726): create_tensor: loading tensor blk.4.layer_output_norm.bias
01-26 11:01:52.929 D/LLAMA_CPP( 8726): create_tensor: loading tensor blk.5.attn_qkv.weight
01-26 11:01:52.929 D/LLAMA_CPP( 8726): create_tensor: loading tensor blk.5.attn_output.weight
01-26 11:01:52.929 D/LLAMA_CPP( 8726): create_tensor: loading tensor blk.5.attn_output_norm.weight
01-26 11:01:52.929 D/LLAMA_CPP( 8726): create_tensor: loading tensor blk.5.attn_output_norm.bias
01-26 11:01:52.929 D/LLAMA_CPP( 8726): create_tensor: loading tensor blk.5.ffn_up.weight
01-26 11:01:52.929 D/LLAMA_CPP( 8726): create_tensor: loading tensor blk.5.ffn_down.weight
01-26 11:01:52.929 D/LLAMA_CPP( 8726): create_tensor: loading tensor blk.5.ffn_gate.weight
01-26 11:01:52.929 D/LLAMA_CPP( 8726): create_tensor: loading tensor blk.5.layer_output_norm.weight
01-26 11:01:52.929 D/LLAMA_CPP( 8726): create_tensor: loading tensor blk.5.layer_output_norm.bias
01-26 11:01:52.929 D/LLAMA_CPP( 8726): create_tensor: loading tensor blk.6.attn_qkv.weight
01-26 11:01:52.929 D/LLAMA_CPP( 8726): create_tensor: loading tensor blk.6.attn_output.weight
01-26 11:01:52.929 D/LLAMA_CPP( 8726): create_tensor: loading tensor blk.6.attn_output_norm.weight
01-26 11:01:52.929 D/LLAMA_CPP( 8726): create_tensor: loading tensor blk.6.attn_output_norm.bias
01-26 11:01:52.929 D/LLAMA_CPP( 8726): create_tensor: loading tensor blk.6.ffn_up.weight
01-26 11:01:52.929 D/LLAMA_CPP( 8726): create_tensor: loading tensor blk.6.ffn_down.weight
01-26 11:01:52.929 D/LLAMA_CPP( 8726): create_tensor: loading tensor blk.6.ffn_gate.weight
01-26 11:01:52.929 D/LLAMA_CPP( 8726): create_tensor: loading tensor blk.6.layer_output_norm.weight
01-26 11:01:52.929 D/LLAMA_CPP( 8726): create_tensor: loading tensor blk.6.layer_output_norm.bias
01-26 11:01:52.929 D/LLAMA_CPP( 8726): create_tensor: loading tensor blk.7.attn_qkv.weight
01-26 11:01:52.929 D/LLAMA_CPP( 8726): create_tensor: loading tensor blk.7.attn_output.weight
01-26 11:01:52.929 D/LLAMA_CPP( 8726): create_tensor: loading tensor blk.7.attn_output_norm.weight
01-26 11:01:52.929 D/LLAMA_CPP( 8726): create_tensor: loading tensor blk.7.attn_output_norm.bias
01-26 11:01:52.929 D/LLAMA_CPP( 8726): create_tensor: loading tensor blk.7.ffn_up.weight
01-26 11:01:52.929 D/LLAMA_CPP( 8726): create_tensor: loading tensor blk.7.ffn_down.weight
01-26 11:01:52.929 D/LLAMA_CPP( 8726): create_tensor: loading tensor blk.7.ffn_gate.weight
01-26 11:01:52.929 D/LLAMA_CPP( 8726): create_tensor: loading tensor blk.7.layer_output_norm.weight
01-26 11:01:52.929 D/LLAMA_CPP( 8726): create_tensor: loading tensor blk.7.layer_output_norm.bias
01-26 11:01:52.929 D/LLAMA_CPP( 8726): create_tensor: loading tensor blk.8.attn_qkv.weight
01-26 11:01:52.929 D/LLAMA_CPP( 8726): create_tensor: loading tensor blk.8.attn_output.weight
01-26 11:01:52.929 D/LLAMA_CPP( 8726): create_tensor: loading tensor blk.8.attn_output_norm.weight
01-26 11:01:52.929 D/LLAMA_CPP( 8726): create_tensor: loading tensor blk.8.attn_output_norm.bias
01-26 11:01:52.929 D/LLAMA_CPP( 8726): create_tensor: loading tensor blk.8.ffn_up.weight
01-26 11:01:52.929 D/LLAMA_CPP( 8726): create_tensor: loading tensor blk.8.ffn_down.weight
01-26 11:01:52.929 D/LLAMA_CPP( 8726): create_tensor: loading tensor blk.8.ffn_gate.weight
01-26 11:01:52.929 D/LLAMA_CPP( 8726): create_tensor: loading tensor blk.8.layer_output_norm.weight
01-26 11:01:52.929 D/LLAMA_CPP( 8726): create_tensor: loading tensor blk.8.layer_output_norm.bias
01-26 11:01:52.929 D/LLAMA_CPP( 8726): create_tensor: loading tensor blk.9.attn_qkv.weight
01-26 11:01:52.929 D/LLAMA_CPP( 8726): create_tensor: loading tensor blk.9.attn_output.weight
01-26 11:01:52.929 D/LLAMA_CPP( 8726): create_tensor: loading tensor blk.9.attn_output_norm.weight
01-26 11:01:52.929 D/LLAMA_CPP( 8726): create_tensor: loading tensor blk.9.attn_output_norm.bias
01-26 11:01:52.929 D/LLAMA_CPP( 8726): create_tensor: loading tensor blk.9.ffn_up.weight
01-26 11:01:52.929 D/LLAMA_CPP( 8726): create_tensor: loading tensor blk.9.ffn_down.weight
01-26 11:01:52.929 D/LLAMA_CPP( 8726): create_tensor: loading tensor blk.9.ffn_gate.weight
01-26 11:01:52.929 D/LLAMA_CPP( 8726): create_tensor: loading tensor blk.9.layer_output_norm.weight
01-26 11:01:52.929 D/LLAMA_CPP( 8726): create_tensor: loading tensor blk.9.layer_output_norm.bias
01-26 11:01:52.929 D/LLAMA_CPP( 8726): create_tensor: loading tensor blk.10.attn_qkv.weight
01-26 11:01:52.929 D/LLAMA_CPP( 8726): create_tensor: loading tensor blk.10.attn_output.weight
01-26 11:01:52.929 D/LLAMA_CPP( 8726): create_tensor: loading tensor blk.10.attn_output_norm.weight
01-26 11:01:52.929 D/LLAMA_CPP( 8726): create_tensor: loading tensor blk.10.attn_output_norm.bias
01-26 11:01:52.929 D/LLAMA_CPP( 8726): create_tensor: loading tensor blk.10.ffn_up.weight
01-26 11:01:52.929 D/LLAMA_CPP( 8726): create_tensor: loading tensor blk.10.ffn_down.weight
01-26 11:01:52.929 D/LLAMA_CPP( 8726): create_tensor: loading tensor blk.10.ffn_gate.weight
01-26 11:01:52.929 D/LLAMA_CPP( 8726): create_tensor: loading tensor blk.10.layer_output_norm.weight
01-26 11:01:52.929 D/LLAMA_CPP( 8726): create_tensor: loading tensor blk.10.layer_output_norm.bias
01-26 11:01:52.929 D/LLAMA_CPP( 8726): create_tensor: loading tensor blk.11.attn_qkv.weight
01-26 11:01:52.929 D/LLAMA_CPP( 8726): create_tensor: loading tensor blk.11.attn_output.weight
01-26 11:01:52.929 D/LLAMA_CPP( 8726): create_tensor: loading tensor blk.11.attn_output_norm.weight
01-26 11:01:52.929 D/LLAMA_CPP( 8726): create_tensor: loading tensor blk.11.attn_output_norm.bias
01-26 11:01:52.929 D/LLAMA_CPP( 8726): create_tensor: loading tensor blk.11.ffn_up.weight
01-26 11:01:52.929 D/LLAMA_CPP( 8726): create_tensor: loading tensor blk.11.ffn_down.weight
01-26 11:01:52.929 D/LLAMA_CPP( 8726): create_tensor: loading tensor blk.11.ffn_gate.weight
01-26 11:01:52.929 D/LLAMA_CPP( 8726): create_tensor: loading tensor blk.11.layer_output_norm.weight
01-26 11:01:52.929 D/LLAMA_CPP( 8726): create_tensor: loading tensor blk.11.layer_output_norm.bias
